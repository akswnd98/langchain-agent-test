{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from ./gemma-2-27b-it-GGUF/gemma-2-27b-it-Q4_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llama_model_loader: - type q4_K:  278 tensors\n",
      "llama_model_loader: - type q6_K:   44 tensors\n",
      "llm_load_vocab: special tokens cache size = 364\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 1\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = all F32\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 15.76 GiB (4.97 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 0.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '15', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp.llama_tokenizer import LlamaTokenizer\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "\n",
    "\n",
    "gguf_path = './gemma-2-27b-it-GGUF/gemma-2-27b-it-Q4_K_L.gguf'\n",
    "tokenizer = LlamaTokenizer.from_ggml_file(gguf_path)\n",
    "llm = LlamaCpp(model_path=gguf_path, max_tokens=4096, temperature=0, n_ctx=4096, n_gpu_layers=-1, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akswnd98/anaconda3/envs/numpy-1.26.4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from chain_items.static import generate_history_passthrough, generate_gemma2_chat_prompt_template2, generate_searx_tool, generate_gmail_tools\n",
    "\n",
    "tools = [\n",
    "  generate_searx_tool()\n",
    "] + generate_gmail_tools()\n",
    "agent = generate_history_passthrough() | generate_gemma2_chat_prompt_template2(tools) | llm | ReActJsonSingleInputOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentAction\n",
    "\n",
    "print(\n",
    "  agent.invoke({\n",
    "    'input': 'who is president of south korea now?',\n",
    "    # 'input': 'please give me a address of myongji university yongin campus.',\n",
    "    'intermediate_steps': [\n",
    "      # (AgentAction('searxng', 'president of south korea', 'Thought: I should search for the recent web results to find the answer.\\n\\nAction: searxng\\n\\nAction Input: \"current president of south korea\"'), 'yoon seok yeol')\n",
    "    ]\n",
    "  })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=5, return_intermediate_steps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m with subject \"Powerful Open Source LLM\"\n",
      "\n",
      "\n",
      "Thought: I need to find the most powerful open-source LLM model using searxng, then use that information to create a draft email and send it.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"searxng\",\n",
      "  \"action_input\": {\"tool_input\": \"most powerful open source LLM\"}\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mNovember 14, 2023 - OPT comprises a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. OPT-175B, one of the most advanced open-source LLMs in the market, is the most powerful brother, with similar performance to GPT-3. Both pre-trained models and the source code are available ...\n",
      "\n",
      "2 weeks ago - Large Language Models (LLMs) have emerged as a cornerstone of today's AI, driving innovations and reshaping the way we interact with technology. As these models become increasingly sophisticated, there's a growing emphasis on democratizing access to them. Open-source models, in particular, ...\n",
      "\n",
      "August 28, 2023 - The download of Falcon 180B is subject to TII's Terms & Conditions and Acceptable Use Policy. Developed by the Technology Innovation Institute under the Apache 2.0 license, the Falcon LLM is another powerful open source LLM that brings a lot of promise in terms of innovation and precision.\n",
      "\n",
      "February 2, 2024 - Falcon LLM is a flagship series developed by the United Arab Emirates' Technology Innovation Institute (TII), a major global research center. It was developed using a custom data pipeline and a distributed training library and offers high performance on multiple Natural Language Processing ...\n",
      "\n",
      "Large Language Models, or LLMs, are advanced computer programs that mimic human-like understanding and generation of text. Recently, open source LLMs have gained popularity, offering the freedom to use, modify, and enhance them, fostering innovation and wider accessibility. This article talks about the top open-source LLMs of 2024, showcasing their key features and benefits.\n",
      "\n",
      "Open-source LLM platforms offer businesses greater flexibility, transparency, and cost savings than closed-source options. Enhanced data security and privacy: With open-source LLMs, ... It’s most powerful version, OPT-175B, boasts a colossal size of 175 billion parameters. OPT was trained on unlabeled text data filtered to contain pre ...\n",
      "\n",
      "The most potent brother is OPT-175B, an open-source LLM that is among the most sophisticated on the market and performs similarly to GPT-3. The public can access both the source code and the pre-trained models. ... While XGen only utilizes 7B parameters for training—much fewer than most powerful open-source LLMs like LLaMA 2 or Falcon ...\n",
      "\n",
      "April 30, 2024 - There’s no need to switch models ... case. Right now, the state of the art in open source ML models caps out at about 70 billion active parameters (Mixtral 8x22B has only 39B active parameters)....\n",
      "\n",
      "April 19, 2024 - Llama 2 is an open source LLM developed by Meta AI, which is one of the most popular open source LLMs. Llama 2 is the first commercially available version of Llama and was released on July 18, 2023. It offers four sizes ranging from 7B to 70B, the pre-training data for Llama 2 consists of 2 ...\n",
      "\n",
      "Developed by the Technology Innovation Institute (TII) in Abu Dhabi, Falcon is a family of open source LLMs that have made significant strides in 2024. The largest model, Falcon-180B, boasts an impressive 180 billion parameters, making it one of the most powerful open source LLMs available.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Thought: Based on the provided information, OPT-175B appears to be the most powerful open-source LLM currently available.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"create_gmail_draft\",\n",
      "  \"action_input\": {\n",
      "    \"message\": \"OPT-175B is considered one of the most powerful open-source LLMs, comparable in performance to GPT-3.\",\n",
      "    \"to\": [\"akswnd98@gmail.com\"],\n",
      "    \"subject\": \"Powerful Open Source LLM\",\n",
      "    \"cc\": [],\n",
      "    \"bcc\": []\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\u001b[33;1m\u001b[1;3mDraft created. Draft Id: r-5972592375894976650\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "AI: Thought: I have successfully created a draft email about the OPT-175B LLM. Now, I should send it.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"send_gmail_message\",\n",
      "  \"action_input\": {\n",
      "    \"message\": \"OPT-175B is considered one of the most powerful open-source LLMs, comparable in performance to GPT-3.\",\n",
      "    \"to\": [\"akswnd98@gmail.com\"],\n",
      "    \"subject\": \"Powerful Open Source LLM\",\n",
      "    \"cc\": [],\n",
      "    \"bcc\": []\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\u001b[38;5;200m\u001b[1;3mMessage sent. Message Id: 190b993bd24fbc65\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "AI: Thought: I have successfully sent the email about the OPT-175B LLM.\n",
      "Final Answer: Email sent!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "output: Email sent!\n"
     ]
    }
   ],
   "source": [
    "res = agent_executor.invoke(input={'input': 'search the most powerful open-source LLM model currently and create draft and send email about it to akswnd98@gmail.com'})\n",
    "\n",
    "print(f'output: {res['output']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
