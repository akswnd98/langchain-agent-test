{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d24d46ef7940b8bf8303520c552fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'device: {device}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../Meta-Llama-3-8B-Instruct')\n",
    "model = AutoModelForCausalLM.from_pretrained('../Meta-Llama-3-8B-Instruct', torch_dtype=torch.float16, device_map={\"\": 0})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "# model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=1024, return_full_text=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_items import PromptValueToChatML, ChatTemplateApplier\n",
    "from chain_items.static import generate_history_passthrough, generate_gemma2_chat_prompt_template, generate_tools  \n",
    "\n",
    "tools = generate_tools()\n",
    "agent = generate_history_passthrough() | generate_gemma2_chat_prompt_template(tools) | PromptValueToChatML() | ChatTemplateApplier(tokenizer=tokenizer) | llm | ReActSingleInputOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input messages=[HumanMessage(content='Answer the following questions as best you can. You have access to the following tools:\\n\\nsearxng(*args, **kargs) - Search searxng for recent web results\\n\\nUse only one of following formats:\\n\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [searxng]\\nAction Input: the input to the action\\n\\nor\\n\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: who is president of south korea now?')]\n",
      "tool='searxng' tool_input='who is the current president of south korea' log='Thought: I should search for the most recent information about the President of South Korea.\\n\\nAction: searxng\\n\\nAction Input: \"who is the current president of south korea\"'\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  agent.invoke({\n",
    "    'input': 'who is president of south korea now?',\n",
    "    # 'input': 'please give me a address of myongji university yongin campus.',\n",
    "    'intermediate_steps': [\n",
    "      # (AgentAction('searxng', 'president of south korea', 'Thought: I should search for the recent web results to find the answer.\\n\\nAction: searxng\\n\\nAction Input: \"current president of south korea\"'), 'yoon seok yeol')\n",
    "    ]\n",
    "  })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=3, return_intermediate_steps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "input messages=[HumanMessage(content='Answer the following questions as best you can. You have access to the following tools:\\n\\nsearxng(*args, **kargs) - Search searxng for recent web results\\n\\nUse only one of following formats:\\n\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [searxng]\\nAction Input: the input to the action\\n\\nor\\n\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: what is reinforcement learning?')]\n",
      "\u001b[32;1m\u001b[1;3mThought: I should search for recent web results on \"reinforcement learning\"\n",
      "\n",
      "Action: searxng\n",
      "\n",
      "Action Input: [\"reinforcement learning\"]\n",
      "\n",
      "(Note: I'll wait for the results from searxng before providing a further response)\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m... reinforcement learning with human feedback (RLHF) scenarios. The labeled data can be used to further train the LLM to improve its performance. Evaluate ...\n",
      "\n",
      "\n",
      "\n",
      "Despite the feature's presence, users were advised by others to wait for further development before diving in. Unsloth Achieves Significant Downloads: The Unsloth team announced hitting 500k monthly model downloads on Hugging Face, thanking the community for the widespread support and usage of Unsloth's finetuning framework. The necessity of ...\n",
      "\n",
      "The reinforcement learning (RL) algorithms can all cope with delays to important reward values (such as those from winning or losing a game). Bigger delays - e.g. thousands of $0$ reward steps between more meaningful values - are often associated with harder problems to solve, so the simpler algorithms might not be practical.\n",
      "\n",
      "Jun 29, 2024 — RLHF Training Puzzles Users: Conversations about Reinforcement Learning from Human Feedback (RLHF) revealed confusion regarding its occasional ...\n",
      "\n",
      "Reinforcement Learning Tutorial with What is Reinforcement Learning, Key Features, What is Q-Learning, Algorithm, Types, The Bellman Equation, Approaches to Implementing Reinforcement Learning etc.\n",
      "\n",
      "... reinforcement learning with human feedback (RLHF) scenarios. The labeled data can be used to further train the LLM to improve its performance. Evaluate ...\n",
      "\n",
      "Course materials from the Machine Learning course of the Vrije Universiteit Amsterdam.\n",
      "\n",
      "April 18, 2023 - A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.\n",
      "\n",
      "Here are ten professional and polite ways to encourage a quick response: 1. Please get back to me as soon as possible. This phrase is professional and straightforward, suitable for formal and semi-formal messages. It conveys urgency without being too pushy.\u001b[0minput messages=[HumanMessage(content='Answer the following questions as best you can. You have access to the following tools:\\n\\nsearxng(*args, **kargs) - Search searxng for recent web results\\n\\nUse only one of following formats:\\n\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [searxng]\\nAction Input: the input to the action\\n\\nor\\n\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: what is reinforcement learning?'), AIMessage(content='Thought: Thought: I should search for recent web results on \"reinforcement learning\"\\n\\nAction: searxng\\n\\nAction Input: [\"reinforcement learning\"]\\n\\n(Note: I\\'ll wait for the results from searxng before providing a further response)'), HumanMessage(content=\"Observation: ... reinforcement learning with human feedback (RLHF) scenarios. The labeled data can be used to further train the LLM to improve its performance. Evaluate ...\\n\\n\\n\\nDespite the feature's presence, users were advised by others to wait for further development before diving in. Unsloth Achieves Significant Downloads: The Unsloth team announced hitting 500k monthly model downloads on Hugging Face, thanking the community for the widespread support and usage of Unsloth's finetuning framework. The necessity of ...\\n\\nThe reinforcement learning (RL) algorithms can all cope with delays to important reward values (such as those from winning or losing a game). Bigger delays - e.g. thousands of $0$ reward steps between more meaningful values - are often associated with harder problems to solve, so the simpler algorithms might not be practical.\\n\\nJun 29, 2024 — RLHF Training Puzzles Users: Conversations about Reinforcement Learning from Human Feedback (RLHF) revealed confusion regarding its occasional ...\\n\\nReinforcement Learning Tutorial with What is Reinforcement Learning, Key Features, What is Q-Learning, Algorithm, Types, The Bellman Equation, Approaches to Implementing Reinforcement Learning etc.\\n\\n... reinforcement learning with human feedback (RLHF) scenarios. The labeled data can be used to further train the LLM to improve its performance. Evaluate ...\\n\\nCourse materials from the Machine Learning course of the Vrije Universiteit Amsterdam.\\n\\nApril 18, 2023 - A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.\\n\\nHere are ten professional and polite ways to encourage a quick response: 1. Please get back to me as soon as possible. This phrase is professional and straightforward, suitable for formal and semi-formal messages. It conveys urgency without being too pushy.\")]\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer: Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. The goal is to maximize the rewards and learn a policy that maps states to actions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "output: Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. The goal is to maximize the rewards and learn a policy that maps states to actions.\n"
     ]
    }
   ],
   "source": [
    "res = agent_executor.invoke(input={'input': 'what is reinforcement learning?'})\n",
    "\n",
    "print(f'output: {res['output']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
