{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272585f975484e1d9d60f312e256b0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'device: {device}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../gemma-2-9b-it')\n",
    "model = AutoModelForCausalLM.from_pretrained('../gemma-2-9b-it', torch_dtype=torch.float16, device_map={'': 0})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "# model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=1024, return_full_text=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_items import PromptValueToChatML, ChatTemplateApplier\n",
    "from chain_items.static import generate_history_passthrough, generate_gemma2_chat_prompt_template, generate_tools  \n",
    "\n",
    "tools = generate_tools()\n",
    "agent = generate_history_passthrough() | generate_gemma2_chat_prompt_template(tools) | PromptValueToChatML() | ChatTemplateApplier(tokenizer=tokenizer) | llm | ReActSingleInputOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input messages=[HumanMessage(content='Answer the following questions as best you can. You have access to the following tools:\\n\\nsearxng(*args, **kargs) - Search searxng for recent web results\\n\\nUse only one of following formats:\\n\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [searxng]\\nAction Input: the input to the action\\n\\nor\\n\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: who is president of south korea now?')]\n",
      "tool='searxng' tool_input='president of south korea \\n' log='Action: searxng\\nAction Input: president of south korea \\n'\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  agent.invoke({\n",
    "    'input': 'who is president of south korea now?',\n",
    "    # 'input': 'please give me a address of myongji university yongin campus.',\n",
    "    'intermediate_steps': [\n",
    "      # (AgentAction('searxng', 'president of south korea', 'Thought: I should search for the recent web results to find the answer.\\n\\nAction: searxng\\n\\nAction Input: \"current president of south korea\"'), 'yoon seok yeol')\n",
    "    ]\n",
    "  })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=3, return_intermediate_steps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "input messages=[HumanMessage(content='Answer the following questions as best you can. You have access to the following tools:\\n\\nsearxng(*args, **kargs) - Search searxng for recent web results\\n\\nUse only one of following formats:\\n\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [searxng]\\nAction Input: the input to the action\\n\\nor\\n\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: what is reinforcement learning?')]\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search the web for information about reinforcement learning. \n",
      "Action: searxng\n",
      "Action Input: reinforcement learning  \n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mReinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals.\u001b[0minput messages=[HumanMessage(content='Answer the following questions as best you can. You have access to the following tools:\\n\\nsearxng(*args, **kargs) - Search searxng for recent web results\\n\\nUse only one of following formats:\\n\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [searxng]\\nAction Input: the input to the action\\n\\nor\\n\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: what is reinforcement learning?'), AIMessage(content='Thought: Thought: I need to search the web for information about reinforcement learning. \\nAction: searxng\\nAction Input: reinforcement learning  \\n'), HumanMessage(content='Observation: Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals.')]\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "output: Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals.\n"
     ]
    }
   ],
   "source": [
    "res = agent_executor.invoke(input={'input': 'what is reinforcement learning?'})\n",
    "\n",
    "print(f'output: {res['output']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
